{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPWnEtCoRgt+2Jj6SViSxD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madhu-sv/asl-sign-language-recognition/blob/master/ASLDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ASL Sign Language Recognition\n",
        "\n",
        "This notebook demonstrates how to train and evaluate a Convolutional Neural Network (CNN) model to recognize American Sign Language (ASL) signs using TensorFlow and Keras.\n",
        "\n",
        "## Step 1: Install Required Libraries\n",
        "\n",
        "First, we need to install the necessary libraries.\n"
      ],
      "metadata": {
        "id": "PmsOcujmXykz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7vE2BB1Xxa4",
        "outputId": "7f423d11-34c2-4c43-9ac3-6b8ac8629413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install tensorflow\n",
        "!pip install matplotlib\n",
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2: Import Libraries\n",
        "Next, import the necessary libraries for building and training the model."
      ],
      "metadata": {
        "id": "4F-CDN8HYIRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.regularizers import l2\n"
      ],
      "metadata": {
        "id": "bD34yZDtYNqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 3: Mount Google Drive\n",
        "Mount Google Drive to access the dataset stored in your Google Drive."
      ],
      "metadata": {
        "id": "j5vxDh36Zx9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "puYfO5OwZ43z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 4: Data Preprocessing\n",
        "Data Augmentation and Preprocessing Functions\n",
        "Define functions to load and preprocess the dataset, including data augmentation."
      ],
      "metadata": {
        "id": "00A_yN1zZ82g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_image(filename, label, num_classes):\n",
        "    img = tf.io.read_file(filename)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, [64, 64])\n",
        "    img = img / 255.0\n",
        "    label = tf.one_hot(label, num_classes)\n",
        "    return img, label\n",
        "\n",
        "def load_dataset(data_dir, batch_size=32, augment=False):\n",
        "    classes = sorted(os.listdir(data_dir))\n",
        "    class_indices = {cls: idx for idx, cls in enumerate(classes)}\n",
        "\n",
        "    filepaths = []\n",
        "    labels = []\n",
        "\n",
        "    for cls in classes:\n",
        "        class_dir = os.path.join(data_dir, cls)\n",
        "        for img in os.listdir(class_dir):\n",
        "            filepaths.append(os.path.join(class_dir, img))\n",
        "            labels.append(class_indices[cls])\n",
        "\n",
        "    filepaths = tf.constant(filepaths)\n",
        "    labels = tf.constant(labels)\n",
        "\n",
        "    num_classes = len(class_indices)\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((filepaths, labels))\n",
        "    dataset = dataset.map(lambda x, y: parse_image(x, y, num_classes), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    if augment:\n",
        "        dataset = dataset.map(lambda x, y: (tf.image.random_flip_left_right(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "        dataset = dataset.map(lambda x, y: (tf.image.random_brightness(x, max_delta=0.2), y), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "        dataset = dataset.map(lambda x, y: (tf.image.random_contrast(x, lower=0.8, upper=1.2), y), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    dataset = dataset.shuffle(buffer_size=len(filepaths))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return dataset, num_classes\n",
        "\n",
        "def create_data_generators(train_dir=None, test_dir=None, batch_size=32):\n",
        "    train_dataset, test_dataset = None, None\n",
        "    num_classes = 0\n",
        "\n",
        "    if train_dir:\n",
        "        train_dataset, num_classes = load_dataset(train_dir, batch_size=batch_size, augment=True)\n",
        "\n",
        "    if test_dir:\n",
        "        test_dataset, num_classes = load_dataset(test_dir, batch_size=batch_size, augment=False)\n",
        "\n",
        "    return train_dataset, test_dataset, num_classes\n"
      ],
      "metadata": {
        "id": "Q2iJDPN5aCDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 5: Build the CNN Model\n",
        "Define the CNN model architecture with regularization techniques to prevent overfitting."
      ],
      "metadata": {
        "id": "seIUB687adZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape=(64, 64, 3), num_classes=29):\n",
        "    \"\"\"\n",
        "    Build a Convolutional Neural Network model.\n",
        "\n",
        "    Args:\n",
        "        input_shape (tuple): Shape of the input images.\n",
        "        num_classes (int): Number of output classes.\n",
        "\n",
        "    Returns:\n",
        "        model: A compiled Keras model.\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Input(shape=input_shape),\n",
        "        Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "        Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "        Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Dropout(0.25),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "OAftr6GgahKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 6: Train the Model\n",
        "Define the training function and use callbacks for early stopping and model checkpointing."
      ],
      "metadata": {
        "id": "-nJwTOfaaw0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_dir, test_dir, epochs=50, batch_size=32):\n",
        "    \"\"\"\n",
        "    Train the CNN model on the ASL dataset using tf.data.\n",
        "\n",
        "    Args:\n",
        "        train_dir (str): Path to the training data directory.\n",
        "        test_dir (str): Path to the test data directory.\n",
        "        epochs (int): Number of epochs to train the model.\n",
        "        batch_size (int): Batch size for training.\n",
        "    \"\"\"\n",
        "    train_dataset, test_dataset, num_classes = create_data_generators(train_dir, test_dir, batch_size=batch_size)\n",
        "\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "    model = build_model(num_classes=num_classes)\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_asl_model.keras', save_best_only=True)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        epochs=epochs,\n",
        "        validation_data=test_dataset,\n",
        "        callbacks=[early_stopping, model_checkpoint]\n",
        "    )\n",
        "\n",
        "    model.save('asl_model.keras')\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(test_dataset)\n",
        "    print(f'Test accuracy: {test_acc}')\n",
        "    return history\n",
        "\n",
        "# Define paths to training and test data\n",
        "train_dir = '/content/drive/MyDrive/asl_data/train'\n",
        "test_dir = '/content/drive/MyDrive/asl_data/test'\n",
        "\n",
        "# Train the model\n",
        "history = train_model(train_dir, test_dir)\n"
      ],
      "metadata": {
        "id": "eJcwE0Cgayd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 7: Evaluate the Model\n",
        "Evaluate the trained model on the test dataset to determine its performance."
      ],
      "metadata": {
        "id": "QXD9dcK0bEAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(test_dir):\n",
        "    \"\"\"\n",
        "    Evaluate the trained CNN model on the ASL test dataset.\n",
        "\n",
        "    Args:\n",
        "        test_dir (str): Path to the test data directory.\n",
        "    \"\"\"\n",
        "    _, test_dataset, num_classes = create_data_generators(test_dir=test_dir, batch_size=32)\n",
        "    model = tf.keras.models.load_model('best_asl_model.keras')\n",
        "\n",
        "    test_loss, test_acc = model.evaluate(test_dataset)\n",
        "    print(f'Test accuracy: {test_acc}')\n",
        "    return test_loss, test_acc\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(test_dir)\n"
      ],
      "metadata": {
        "id": "0jEePxn4bFoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 8: Visualize Training History\n",
        "Visualize the training and validation accuracy and loss over epochs."
      ],
      "metadata": {
        "id": "d3p-DjzibOIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plot the training history.\n",
        "\n",
        "    Args:\n",
        "        history: History object from model training.\n",
        "    \"\"\"\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "    plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Plot training history\n",
        "plot_training_history(history)\n"
      ],
      "metadata": {
        "id": "o4Do-UzKbT-b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}